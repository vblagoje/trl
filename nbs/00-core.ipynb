{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "> A set of utility functions used throughout the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers.generation_utils import GenerationMixin, top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def flatten_dict(nested, sep='/'):\n",
    "    \"\"\"Flatten dictionary and concatenate nested keys with separator.\"\"\"\n",
    "    def rec(nest, prefix, into):\n",
    "        for k, v in nest.items():\n",
    "            if sep in k:\n",
    "                raise ValueError(f\"separator '{sep}' not allowed to be in key '{k}'\")\n",
    "            if isinstance(v, collections.Mapping):\n",
    "                rec(v, prefix + k + sep, into)\n",
    "            else:\n",
    "                into[prefix + k] = v\n",
    "    flat = {}\n",
    "    rec(nested, '', flat)\n",
    "    return flat\n",
    "\n",
    "def stack_dicts(stats_dicts):\n",
    "    \"\"\"Stack the values of a dict.\"\"\"\n",
    "    results = dict()\n",
    "    for k in stats_dicts[0]:\n",
    "        stats_list = [torch.flatten(d[k]) for d in stats_dicts]\n",
    "        results[k] = torch.stack(stats_list)\n",
    "    return results\n",
    "\n",
    "def add_suffix(input_dict, suffix):\n",
    "    \"\"\"Add suffix to dict keys.\"\"\"\n",
    "    return dict((k + suffix, v) for k,v in input_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def pad_to_size(tensor, size, dim=1, padding=50256):\n",
    "    \"\"\"Pad tensor to size.\"\"\"\n",
    "    t_size = tensor.size()[dim]\n",
    "    if t_size==size:\n",
    "        return tensor\n",
    "    else:\n",
    "        return torch.nn.functional.pad(tensor, (0,size-t_size), 'constant', padding)\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=2)\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy\n",
    "\n",
    "\n",
    "def whiten(values, shift_mean=True):\n",
    "    \"\"\"Whiten values.\"\"\"\n",
    "    mean, var = torch.mean(values), torch.var(values)\n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    if not shift_mean:\n",
    "        whitened += mean\n",
    "    return whitened\n",
    "\n",
    "def clip_by_value(x, tensor_min, tensor_max):\n",
    "    \"\"\"\n",
    "    Tensor extenstion to torch.clamp\n",
    "    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713\n",
    "    \"\"\"\n",
    "    clipped = torch.max(torch.min(x, tensor_max), tensor_min)\n",
    "    return clipped\n",
    "\n",
    "def entropy_from_logits(logits):\n",
    "    \"\"\"Calculate entropy from logits.\"\"\"\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd*logits, axis=-1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def average_torch_dicts(list_of_dicts):\n",
    "    \"\"\"Average values of a list of dicts wiht torch tensors.\"\"\"\n",
    "    average_dict = dict()\n",
    "    for key in list_of_dicts[0].keys():\n",
    "        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)\n",
    "    return average_dict\n",
    "\n",
    "def stats_to_np(stats_dict):\n",
    "    \"\"\"Cast all torch.tensors in dict to numpy arrays.\"\"\"\n",
    "    new_dict = dict()\n",
    "    for k, v in stats_dict.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            new_dict[k] = v.detach().cpu().numpy()\n",
    "        else:\n",
    "            new_dict[k] = v\n",
    "        if np.isscalar(new_dict[k]):\n",
    "            new_dict[k] = float(new_dict[k])\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def build_bert_batch_from_txt(text_list, tokenizer, device):\n",
    "    \"\"\"Create token id and attention mask tensors from text list for BERT classification.\"\"\"\n",
    "    \n",
    "    # tokenize\n",
    "    tensors = [tokenizer.encode(txt, return_tensors=\"pt\").to(device) for txt in text_list]\n",
    "    \n",
    "    # find max length to pad to\n",
    "    max_len = max([t.size()[1] for t in tensors])\n",
    "    \n",
    "    # get padded tensors and attention masks\n",
    "    # (attention masks make bert ignore padding)\n",
    "    padded_tensors = []\n",
    "    attention_masks = []\n",
    "    for tensor in tensors:\n",
    "        attention_mask = torch.ones(tensor.size(), device=device)\n",
    "        padded_tensors.append(pad_to_size(tensor, max_len, padding=0))\n",
    "        attention_masks.append(pad_to_size(attention_mask, max_len, padding=0))\n",
    "    \n",
    "    # stack all tensors\n",
    "    padded_tensors = torch.cat(padded_tensors)\n",
    "    attention_masks = torch.cat(attention_masks)  \n",
    "    \n",
    "    return padded_tensors, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class RLMixin(GenerationMixin):\n",
    "\n",
    "    def respond_to_batch(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,\n",
    "            max_length: Optional[int] = None,\n",
    "            top_k: Optional[int] = 0,\n",
    "            top_p: Optional[float] = 1.0,\n",
    "            bos_token_id: Optional[int] = None,\n",
    "            pad_token_id: Optional[int] = None,\n",
    "            eos_token_id: Optional[int] = None,\n",
    "            num_return_sequences: Optional[int] = None,\n",
    "            decoder_start_token_id: Optional[int] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            output_scores: Optional[bool] = None,\n",
    "            return_dict_in_generate: Optional[bool] = None,\n",
    "            **model_kwargs,\n",
    "    ):\n",
    "        \"\"\"Sample text from language model.\"\"\"\n",
    "\n",
    "        num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "        )\n",
    "\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"output_attentions\"] = output_attentions\n",
    "        model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "\n",
    "        if input_ids is None:\n",
    "            # init `input_ids` with bos_token_id\n",
    "            input_ids = self._prepare_input_ids_for_generation(bos_token_id)\n",
    "\n",
    "        if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "            # init `attention_mask` depending on `pad_token_id`\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "                input_ids, pad_token_id, eos_token_id\n",
    "            )\n",
    "\n",
    "        # special case if pad_token_id is not defined\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            print(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            pad_token_id = eos_token_id\n",
    "\n",
    "        # Storing encoder_input_ids for logits_processor that could use them\n",
    "        encoder_input_ids = input_ids if self.config.is_encoder_decoder else None\n",
    "\n",
    "        if self.config.is_encoder_decoder:\n",
    "            # add encoder_outputs to model_kwargs\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n",
    "\n",
    "            # set input_ids as decoder_input_ids\n",
    "            if \"decoder_input_ids\" in model_kwargs:\n",
    "                input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "            else:\n",
    "                input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "                    input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id\n",
    "                )\n",
    "\n",
    "            if \"encoder_outputs\" not in model_kwargs or not isinstance(model_kwargs[\"encoder_outputs\"], ModelOutput):\n",
    "                raise ValueError(\"Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.\")\n",
    "\n",
    "        if input_ids.shape[-1] >= max_length:\n",
    "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "            print(\n",
    "                f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.\"\n",
    "                \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "            )\n",
    "\n",
    "        # expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "            input_ids,\n",
    "            expand_size=num_return_sequences,\n",
    "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "\n",
    "        # init sequence length tensors\n",
    "        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(\n",
    "            input_ids, max_length\n",
    "        )\n",
    "\n",
    "        gen_length = 0\n",
    "        # auto-regressive generation\n",
    "        while cur_len < max_length:\n",
    "            # prepare model inputs\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs[0][:, -1, :]\n",
    "            next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            # Sample\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "            # add code that transfomers next_tokens to tokens_to_add\n",
    "            if eos_token_id is not None:\n",
    "                assert pad_token_id is not None, \"If eos_token_id is defined, make sure that pad_token_id is defined.\"\n",
    "                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)\n",
    "\n",
    "            # add token and increase length by one\n",
    "            input_ids = torch.cat([input_ids, next_tokens.unsqueeze(-1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "            gen_length += 1\n",
    "\n",
    "            # update sequence length\n",
    "            if eos_token_id is not None:\n",
    "                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(\n",
    "                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\n",
    "                )\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sequences.max() == 0:\n",
    "                break\n",
    "\n",
    "            # update model kwargs\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "        return input_ids[:, -gen_length:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

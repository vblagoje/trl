# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01-gpt2-with-value-head.ipynb (unless otherwise specified).

__all__ = ['ValueHead', 'GPT2HeadWithValueModel', 'respond_to_batch']

# Cell

import torch
import torch.nn.functional as F
from typing import *
from torch import nn
from torch.nn import Identity
from transformers import GPT2Model, GPT2PreTrainedModel
from transformers import top_k_top_p_filtering

# Cell


class ValueHead(nn.Module):
    """The ValueHead class implements a head for GPT2 that returns a scalar for each output token."""
    def __init__(self, config):
        super().__init__()
        self.detach_head = False
        self.summary_type = config.summary_type if hasattr(config, "summary_type") else "last"
        if self.summary_type == "attn":
            raise NotImplementedError

        self.summary = Identity()
        if hasattr(config, "summary_use_proj") and config.summary_use_proj:
            if hasattr(config, "summary_proj_to_labels") and config.summary_proj_to_labels and config.num_labels > 0:
                num_classes = config.num_labels
            else:
                num_classes = config.hidden_size
            self.summary = nn.Linear(config.hidden_size, num_classes)

        self.activation = Identity()
        if hasattr(config, "summary_activation") and config.summary_activation == "tanh":
            self.activation = nn.Tanh()

        self.first_dropout = Identity()
        if hasattr(config, "summary_first_dropout") and config.summary_first_dropout > 0:
            self.first_dropout = nn.Dropout(config.summary_first_dropout)

        self.last_dropout = Identity()
        if hasattr(config, "summary_last_dropout") and config.summary_last_dropout > 0:
            self.last_dropout = nn.Dropout(config.summary_last_dropout)

        self.flatten = nn.Flatten()

    def forward(self, hidden_states, cls_index=None):
        if self.detach_head:
            output = hidden_states.detach()
        else:
            output = hidden_states
        output = self.first_dropout(output)
        output = self.summary(output)
        output = self.activation(output)
        output = self.last_dropout(output)

        return output

# Cell

class GPT2HeadWithValueModel(GPT2PreTrainedModel):
    """The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head."""
    def __init__(self, config):
        super().__init__(config)
        config.num_labels = 1
        self.transformer = GPT2Model(config)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.v_head = ValueHead(config)

        self.init_weights()

    def get_output_embeddings(self):
        return self.lm_head

    def detach_value_head(self):
        self.v_head.detach_head = True

    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):

        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_hidden_states=True,
            return_dict=True
        )

        hidden_states = transformer_outputs[0]

        lm_logits = self.lm_head(hidden_states)
        value = self.v_head(hidden_states).squeeze(-1)
        transformer_outputs["logits"] = lm_logits
        transformer_outputs["state_values"] = value
        return transformer_outputs

# Cell

def respond_to_batch(model, input_ids,
                     pad_token_id:Optional[int] = None,
                     bos_token_id:Optional[int] = None,
                     eos_token_id:Optional[int] = None,
                     txt_len=128, top_k=0, top_p=1.0,
                     **model_kwargs):
    
    """Sample text from language model."""

    model_kwargs["output_attentions"] = False
    model_kwargs["output_hidden_states"] = False

    pad_token_id = pad_token_id if pad_token_id is not None else model.config.pad_token_id
    bos_token_id = bos_token_id if bos_token_id is not None else model.config.bos_token_id
    eos_token_id = eos_token_id if eos_token_id is not None else model.config.eos_token_id

    if model.config.is_encoder_decoder:
        # add encoder_outputs to model_kwargs
        model_kwargs = _prepare_encoder_decoder_kwargs_for_generation(model, input_ids, model_kwargs)

        # set input_ids as decoder_input_ids
        if "decoder_input_ids" in model_kwargs:
            input_ids = model_kwargs.pop("decoder_input_ids")
        else:
            input_ids = _prepare_decoder_input_ids_for_generation(model,
                input_ids,
                decoder_start_token_id=None,
                bos_token_id=model.config.bos_token_id
            )

    # init sequence length tensors
    sequence_lengths, unfinished_sequences, cur_len = _init_sequence_length_for_generation(
            input_ids, txt_len
    )

    while cur_len < txt_len:

        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)
        outputs = model(**model_inputs, return_dict=True)
        next_token_logits = outputs.logits[:, -1, :]
        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)
        # Sample
        probs = F.softmax(next_token_logits, dim=-1)
        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)

        # add code that transfomers next_tokens to tokens_to_add
        if eos_token_id is not None:
            assert pad_token_id is not None, "If eos_token_id is defined, make sure that pad_token_id is defined."
            next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)

        # add token and increase length by one
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        cur_len = cur_len + 1

        # update sequence length
        if eos_token_id is not None:
            sequence_lengths, unfinished_sequences = _update_seq_length_for_generation(
                sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id
            )

        # stop when there is a </s> in each sentence, or if we exceed the maximul length
        if unfinished_sequences.max() == 0:
            break

        # update model kwargs
        model_kwargs = _update_model_kwargs_for_generation(
            outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder
        )

    return input_ids


def _prepare_encoder_decoder_kwargs_for_generation(
        model, input_ids: torch.LongTensor, model_kwargs
    ) -> Dict[str, Any]:
        # retrieve encoder hidden states
        encoder = model.get_encoder()
        encoder_kwargs = {
            argument: value for argument, value in model_kwargs.items() if not argument.startswith("decoder_")
        }
        model_kwargs["encoder_outputs"]: Dict = encoder(input_ids, return_dict=True, **encoder_kwargs)
        return model_kwargs


def _prepare_decoder_input_ids_for_generation(model, input_ids: torch.LongTensor,
                                              decoder_start_token_id: int = None,
                                              bos_token_id: int = None
    ) -> torch.LongTensor:
        decoder_start_token_id = _get_decoder_start_token_id(model, decoder_start_token_id, bos_token_id)
        decoder_input_ids = (
            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)
            * decoder_start_token_id
        )
        return decoder_input_ids


def _get_decoder_start_token_id(model, decoder_start_token_id: int = None, bos_token_id: int = None) -> int:
    decoder_start_token_id = (
        decoder_start_token_id if decoder_start_token_id is not None else model.config.decoder_start_token_id
    )
    bos_token_id = bos_token_id if bos_token_id is not None else model.config.bos_token_id

    if decoder_start_token_id is not None:
        return decoder_start_token_id
    elif (
        hasattr(model.config, "decoder")
        and hasattr(model.config.decoder, "decoder_start_token_id")
        and model.config.decoder.decoder_start_token_id is not None
    ):
        return model.config.decoder.decoder_start_token_id
    elif bos_token_id is not None:
        return bos_token_id
    elif (
        hasattr(model.config, "decoder")
        and hasattr(model.config.decoder, "bos_token_id")
        and model.config.decoder.bos_token_id is not None
    ):
        return model.config.decoder.bos_token_id
    raise ValueError(
        "`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation."
    )


def _init_sequence_length_for_generation(
        input_ids: torch.LongTensor, max_length: int
) -> Tuple[torch.Tensor, torch.Tensor, int]:
    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
    sequence_lengths = input_ids.new(input_ids.shape[0]).fill_(max_length)

    cur_len = input_ids.shape[-1]
    return sequence_lengths, unfinished_sequences, cur_len


def _update_seq_length_for_generation(
        sequence_lengths: torch.LongTensor,
        unfinished_sequences: torch.LongTensor,
        cur_len: int,
        is_eos_in_next_token: torch.BoolTensor,
) -> Tuple[torch.LongTensor, torch.LongTensor]:
    # check if sentence is not finished yet
    is_sent_unfinished = unfinished_sequences.mul(is_eos_in_next_token.long()).bool()

    # update sentence length
    sequence_lengths = sequence_lengths.masked_fill(is_sent_unfinished, cur_len)
    unfinished_sequences = unfinished_sequences.mul((~is_eos_in_next_token).long())
    return sequence_lengths, unfinished_sequences


def _update_model_kwargs_for_generation(
        outputs: OrderedDict, model_kwargs: Dict[str, Any], is_encoder_decoder: bool = False
) -> Dict[str, Any]:
    # update past
    if "past_key_values" in outputs:
        model_kwargs["past"] = outputs.past_key_values
    elif "mems" in outputs:
        model_kwargs["past"] = outputs.mems
    elif "past_buckets_states" in outputs:
        model_kwargs["past"] = outputs.past_buckets_states
    else:
        model_kwargs["past"] = None

    # update token_type_ids with last value
    if "token_type_ids" in model_kwargs:
        token_type_ids = model_kwargs["token_type_ids"]
        model_kwargs["token_type_ids"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)

    # update attention mask
    if not is_encoder_decoder:
        if "attention_mask" in model_kwargs:
            attention_mask = model_kwargs["attention_mask"]
            model_kwargs["attention_mask"] = torch.cat(
                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1
            )

    return model_kwargs

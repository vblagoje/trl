# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06-t5-with-value-head.ipynb (unless otherwise specified).

__all__ = ['T5ValueHead', 'T5HeadWithValueModel']

# Cell
from transformers import T5Model, T5PreTrainedModel, AutoTokenizer,T5ForConditionalGeneration
from transformers import top_k_top_p_filtering
from torch import nn
from torch.nn import Identity
import torch.nn.functional as F
import torch
from .core import RLMixin
from collections import OrderedDict

# Cell

class T5ValueHead(nn.Module):
    """The ValueHead class implements a head for T5 that returns a scalar for each output token."""
    def __init__(self, config):
        super().__init__()
        self.state_representation = nn.Linear(config.vocab_size, 1)

    def forward(self, hidden_states):
        output = self.state_representation(hidden_states)
        return output

# Cell

class T5HeadWithValueModel(T5ForConditionalGeneration, RLMixin):
    """The T5HeadWithValueModel class implements a T5 language model with a secondary, scalar head."""
    def __init__(self, config):
        super().__init__(config)
        self.v_head = T5ValueHead(config)

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        decoder_input_ids=None,
        decoder_attention_mask=None,
        head_mask=None,
        decoder_head_mask=None,
        encoder_outputs=None,
        past_key_values=None,
        inputs_embeds=None,
        decoder_inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None
    ):

        transformer_output = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            head_mask=head_mask,
            decoder_head_mask=decoder_head_mask,
            encoder_outputs=encoder_outputs,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            decoder_inputs_embeds=decoder_inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=True,
            return_dict=True
        )


        value = self.v_head(transformer_output[0]).squeeze(-1)

        return transformer_output